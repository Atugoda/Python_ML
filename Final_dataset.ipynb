{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = 'Final_Dataset.csv'  # Update this path as per your file location\n",
    "data = pd.read_csv(\"C:/Users/LENOVO/Desktop/Final_Dataset.csv\", encoding='latin1')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assume the last column is the target variable; adjust if needed\n",
    "target_column = data.columns[-1]\n",
    "\n",
    "X = data.drop(columns=[target_column])  # Features\n",
    "y = data[target_column]  # Target variable\n",
    "\n",
    "# Step 2: Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Preprocessing\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),  # Handle missing values\n",
    "    (\"scaler\", StandardScaler())                 # Standardize the data\n",
    "])\n",
    "\n",
    "X_train = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test = preprocessing_pipeline.transform(X_test)\n",
    "\n",
    "# Step 4: Define Ensemble Models\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "gb_model = GradientBoostingClassifier(random_state=42, n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[('rf', rf_model), ('gb', gb_model)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Step 5: Train the Model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Predictions and Evaluation\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 7: Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Feature Importance (Random Forest only, for interpretability)\n",
    "try:\n",
    "    feature_importances = rf_model.feature_importances_\n",
    "    feature_names = data.drop(columns=[target_column]).columns\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_names, feature_importances, color='skyblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Feature Importance - Random Forest')\n",
    "    plt.show()\n",
    "except AttributeError:\n",
    "    print(\"Feature importances are not available for this model.\")\n",
    "\n",
    "# Step 9: ROC Curve (only for binary classification)\n",
    "if len(np.unique(y)) == 2:\n",
    "    y_prob = ensemble_model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ROC Curve is not applicable for multiclass classification.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"C:/Users/LENOVO/Desktop/bank.csv\"  # Path to your dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Inspect columns (already done)\n",
    "print(data.columns)\n",
    "\n",
    "# Step 3: Data Preprocessing\n",
    "target_column = 'deposit'  # The target variable column name\n",
    "X = data.drop(columns=[target_column])  # Features\n",
    "y = data[target_column]  # Target variable\n",
    "\n",
    "# Handling missing data (if any)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputing missing values with the most frequent value for categorical columns and mean for numerical columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Imputation for numerical features (mean)\n",
    "numerical_imputer = SimpleImputer(strategy='mean')\n",
    "X[numerical_features] = numerical_imputer.fit_transform(X[numerical_features])\n",
    "\n",
    "# Imputation for categorical features (most frequent)\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X[categorical_features] = categorical_imputer.fit_transform(X[categorical_features])\n",
    "\n",
    "# Step 4: Feature Scaling and Encoding\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression  # Correct import for LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessor for scaling numerical features and encoding categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Step 5: Build and Train Ensemble Model\n",
    "ensemble_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', VotingClassifier(estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('lr', LogisticRegression())\n",
    "    ], voting='hard'))\n",
    "])\n",
    "\n",
    "# Step 6: Train the model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predictions and Evaluation\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Accuracy and classification report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:/Users/LENOVO/Desktop/bank.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 3: Data Preprocessing\n",
    "target_column = 'deposit'  # Adjusted target column based on the dataset\n",
    "X = data.drop(columns=[target_column])  # Features\n",
    "y = data[target_column]  # Target variable\n",
    "\n",
    "# Handling missing data (if any)\n",
    "X = X.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "X = X.fillna(X.mean())  # For numerical columns\n",
    "\n",
    "# Convert categorical variables into a suitable format using OneHotEncoder\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocessing pipeline for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Step 5: Build and Train Ensemble Model\n",
    "ensemble_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', VotingClassifier(estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('lr', LogisticRegression(max_iter=300, random_state=42))  # Increased iterations\n",
    "    ], voting='hard'))\n",
    "])\n",
    "\n",
    "# Step 6: Train the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predictions and Evaluation\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Accuracy and Classification Report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "y_pred_prob = ensemble_model.predict_proba(X_test)[:, 1]  # Probabilities for the 'yes' class\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = r\"C:/Users/LENOVO/Desktop/bank.csv\"  # Adjust this path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Check for missing values or any other issues\n",
    "print(\"Missing values per column:\\n\", data.isnull().sum())\n",
    "\n",
    "# Clean the data (strip whitespace from string columns and handle missing data)\n",
    "data = data.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "data.fillna(data.mean(), inplace=True)  # For numerical columns\n",
    "\n",
    "# Step 3: Preprocessing\n",
    "target_column = 'deposit'  # This is the column to predict\n",
    "\n",
    "# Features and target variable\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Split into categorical and numerical features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Step 4: Build the Ensemble Model\n",
    "ensemble_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', VotingClassifier(estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('lr', LogisticRegression(max_iter=300, random_state=42))  # Increased iterations for LR\n",
    "    ], voting='hard'))\n",
    "])\n",
    "\n",
    "# Step 5: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Train the Ensemble Model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Model Evaluation\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Accuracy and Classification Report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix Visualization\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "y_pred_prob = ensemble_model.predict_proba(X_test)[:, 1]  # Probabilities for the 'yes' class\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = r\"C:/Users/LENOVO/Desktop/bank.csv\"  # Adjust this path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Check for missing values or any other issues\n",
    "print(\"Missing values per column:\\n\", data.isnull().sum())\n",
    "\n",
    "# Step 3: Preprocessing\n",
    "target_column = 'deposit'  # This is the column to predict\n",
    "\n",
    "# Features and target variable\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Split into categorical and numerical features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Step 4: Build the Ensemble Model\n",
    "ensemble_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', VotingClassifier(estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('lr', LogisticRegression(max_iter=300, random_state=42))  # Increased iterations for LR\n",
    "    ], voting='hard'))\n",
    "])\n",
    "\n",
    "# Step 5: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Train the Ensemble Model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Model Evaluation\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Accuracy and Classification Report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix Visualization\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "y_pred_prob = ensemble_model.predict_proba(X_test)[:, 1]  # Probabilities for the 'yes' class\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = r\"C:/Users/LENOVO/Desktop/bank.csv\"  # Adjust this path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Check for missing values or any other issues\n",
    "print(\"Missing values per column:\\n\", data.isnull().sum())\n",
    "\n",
    "# Step 3: Preprocessing\n",
    "target_column = 'deposit'  # This is the column to predict\n",
    "\n",
    "# Features and target variable\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Split into categorical and numerical features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Step 4: Build the Ensemble Model\n",
    "ensemble_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', VotingClassifier(estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('lr', LogisticRegression(max_iter=300, random_state=42))  # Increased iterations for LR\n",
    "    ], voting='soft'))  # Use soft voting\n",
    "])\n",
    "\n",
    "# Step 5: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Train the Ensemble Model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Model Evaluation\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Accuracy and Classification Report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix Visualization\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "y_pred_prob = ensemble_model.predict_proba(X_test)[:, 1]  # Probabilities for the 'yes' class\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = r\"C:/Users/LENOVO/Desktop/bank.csv\"  # Adjust this path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Check for missing values or any other issues\n",
    "print(\"Missing values per column:\\n\", data.isnull().sum())\n",
    "\n",
    "# Step 3: Preprocessing\n",
    "target_column = 'deposit'  # This is the column to predict\n",
    "\n",
    "# Features and target variable\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Split into categorical and numerical features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Step 4: Build the Ensemble Model\n",
    "ensemble_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', VotingClassifier(estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('lr', LogisticRegression(max_iter=300, random_state=42))  # Increased iterations for LR\n",
    "    ], voting='soft'))  # Use soft voting\n",
    "])\n",
    "\n",
    "# Step 5: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Train the Ensemble Model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Model Evaluation\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Accuracy and Classification Report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix Visualization\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = r\"C:/Users/LENOVO/Desktop/bank.csv\"  # Adjust this path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Check for missing values or any other issues\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing values per column before handling:\\n\", missing_values)\n",
    "\n",
    "# Print columns that had missing values\n",
    "columns_with_missing = missing_values[missing_values > 0]\n",
    "if not columns_with_missing.empty:\n",
    "    print(\"\\nColumns with missing values before setting to zero:\")\n",
    "    print(columns_with_missing)\n",
    "\n",
    "# Step 3: Preprocessing\n",
    "target_column = 'deposit'  # This is the column to predict\n",
    "\n",
    "# Features and target variable\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Step 4: Handle missing values (set them to zero)\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Split into categorical and numerical features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Step 5: Build the Ensemble Model\n",
    "ensemble_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', VotingClassifier(estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('lr', LogisticRegression(max_iter=300, random_state=42))  # Increased iterations for LR\n",
    "    ], voting='soft'))  # Use soft voting\n",
    "])\n",
    "\n",
    "# Step 6: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Train the Ensemble Model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Model Evaluation\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Accuracy and Classification Report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix Visualization\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:/Users/LENOVO/Desktop/bank.csv\"  # Adjust this path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\\n\", data.isnull().sum())\n",
    "\n",
    "# Preprocessing\n",
    "target_column = 'deposit'  # This is the column to predict\n",
    "\n",
    "# Features and target variable\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Split into categorical and numerical features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', StandardScaler(), numerical_features),\n",
    "                  ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)])\n",
    "\n",
    "# Ensemble Model Setup\n",
    "ensemble_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', VotingClassifier(estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42, class_weight='balanced')),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('lr', LogisticRegression(max_iter=300, random_state=42))], voting='soft'))\n",
    "])\n",
    "\n",
    "# Step 1: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Hyperparameter Tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__rf__n_estimators': [100, 200],\n",
    "    'classifier__rf__max_depth': [10, 20, None],\n",
    "    'classifier__gb__learning_rate': [0.01, 0.1],\n",
    "    'classifier__gb__n_estimators': [100, 200],\n",
    "    'classifier__lr__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(ensemble_model, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from grid search\n",
    "print(\"Best parameters from GridSearchCV:\\n\", grid_search.best_params_)\n",
    "\n",
    "# Step 3: Model Evaluation\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Accuracy and Classification Report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix Visualization\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "y_pred_prob = grid_search.best_estimator_.predict_proba(X_test)[:, 1]  # Probabilities for the 'yes' class\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Cross-Validation Score\n",
    "cv_score = cross_val_score(grid_search.best_estimator_, X, y, cv=5)\n",
    "print(\"Cross-validation Accuracy:\", cv_score.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = r\"C:/Users/LENOVO/Desktop/Final_Dataset.csv\"  # Update this path as per your file location\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Assume the last column is the target variable; adjust if needed\n",
    "target_column = data.columns[-1]\n",
    "\n",
    "X = data.drop(columns=[target_column])  # Features\n",
    "y = data[target_column]  # Target variable\n",
    "\n",
    "# Step 2: Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Preprocessing for numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 4: Define Models\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "gb_model = GradientBoostingClassifier(random_state=42, n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "ensemble_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", VotingClassifier(\n",
    "        estimators=[(\"rf\", rf_model), (\"gb\", gb_model)],\n",
    "        voting=\"soft\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Step 5: Train the Model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Predictions and Evaluation\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance (Random Forest only, for interpretability)\n",
    "rf_model.fit(preprocessor.fit_transform(X_train), y_train)\n",
    "feature_importances = rf_model.feature_importances_\n",
    "feature_names = (\n",
    "    list(numeric_features) + \n",
    "    list(preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features))\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, feature_importances, color=\"skyblue\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance - Random Forest\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve (only for binary classification)\n",
    "if len(np.unique(y)) == 2:\n",
    "    y_prob = ensemble_model.named_steps[\"classifier\"].predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ROC Curve is not applicable for multiclass classification.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = r\"C:/Users/LENOVO/Desktop/Final_Dataset.csv\"  # Update this path as per your file location\n",
    "\n",
    "# Attempt to read the file with specified encoding\n",
    "try:\n",
    "    data = pd.read_csv(file_path, encoding=\"latin1\")  # Adjust encoding if needed\n",
    "except UnicodeDecodeError:\n",
    "    print(\"UnicodeDecodeError encountered. Trying another encoding...\")\n",
    "    data = pd.read_csv(file_path, encoding=\"utf-8\", errors=\"replace\")  # Fallback if latin1 doesn't work\n",
    "\n",
    "# Assume the last column is the target variable; adjust if needed\n",
    "target_column = data.columns[-1]\n",
    "\n",
    "X = data.drop(columns=[target_column])  # Features\n",
    "y = data[target_column]  # Target variable\n",
    "\n",
    "# Step 2: Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Preprocessing for numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 4: Define Models\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "gb_model = GradientBoostingClassifier(random_state=42, n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "ensemble_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", VotingClassifier(\n",
    "        estimators=[(\"rf\", rf_model), (\"gb\", gb_model)],\n",
    "        voting=\"soft\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Step 5: Train the Model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Predictions and Evaluation\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance (Random Forest only, for interpretability)\n",
    "rf_model.fit(preprocessor.fit_transform(X_train), y_train)\n",
    "feature_importances = rf_model.feature_importances_\n",
    "feature_names = (\n",
    "    list(numeric_features) + \n",
    "    list(preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features))\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, feature_importances, color=\"skyblue\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance - Random Forest\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve (only for binary classification)\n",
    "if len(np.unique(y)) == 2:\n",
    "    y_prob = ensemble_model.named_steps[\"classifier\"].predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ROC Curve is not applicable for multiclass classification.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = r\"C:/Users/LENOVO/Desktop/Final_Dataset.csv\"  # Update this path as per your file location\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Assume the last column is the target variable; adjust if needed\n",
    "target_column = data.columns[-1]\n",
    "\n",
    "X = data.drop(columns=[target_column])  # Features\n",
    "y = data[target_column]  # Target variable\n",
    "\n",
    "# Step 2: Check if the target variable is categorical (for classification)\n",
    "# Convert to categorical if it's not (e.g., by binning continuous values)\n",
    "if y.dtype != \"object\":\n",
    "    print(\"Target is continuous, binning into categories.\")\n",
    "    y = pd.cut(y, bins=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Step 3: Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Preprocessing for numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),  # Handle missing values\n",
    "            (\"scaler\", StandardScaler())  # Standardize the data\n",
    "        ]), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 5: Define Models for Classification\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "gb_model = GradientBoostingClassifier(random_state=42, n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "ensemble_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", VotingClassifier(\n",
    "        estimators=[(\"rf\", rf_model), (\"gb\", gb_model)],\n",
    "        voting=\"soft\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Step 6: Train the Model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predictions and Evaluation\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance (Random Forest only, for interpretability)\n",
    "rf_model.fit(preprocessor.fit_transform(X_train), y_train)\n",
    "feature_importances = rf_model.feature_importances_\n",
    "feature_names = (\n",
    "    list(numeric_features) +\n",
    "    list(preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features))\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, feature_importances, color=\"skyblue\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance - Random Forest\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve (only for binary classification)\n",
    "if len(np.unique(y)) == 2:\n",
    "    y_prob = ensemble_model.named_steps[\"classifier\"].predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ROC Curve is not applicable for multiclass classification.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the dataset\n",
    "data_path = \"C:/Users/LENOVO/Desktop/bank.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Inspect the dataset\n",
    "print(\"Dataset Shape:\", data.shape)\n",
    "print(\"Dataset Columns:\", data.columns)\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = r\"C:\\Users\\LENOVO\\Desktop\\bank.csv\"  # Update this path with your actual file location\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Inspect the dataset\n",
    "print(\"Dataset Shape:\", data.shape)\n",
    "print(\"Dataset Columns:\", data.columns)\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Step 3: Data Preprocessing\n",
    "# Assume the target column is 'y', which is the subscription to the term deposit (yes/no)\n",
    "target_column = 'y'\n",
    "X = data.drop(columns=[target_column])  # Features\n",
    "y = data[target_column]  # Target variable\n",
    "\n",
    "# Handling missing data (if any)\n",
    "# Imputing missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")  # You can change strategy if needed\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Step 4: Encode categorical features and scale numerical features\n",
    "# Define the numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocessing for numerical data (imputation + scaling)\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Imputation\n",
    "    ('scaler', StandardScaler())  # Standardization (scaling)\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data (imputation + one-hot encoding)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputation\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encoding\n",
    "])\n",
    "\n",
    "# Combine preprocessing for both numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: Create an Ensemble Model (Voting Classifier)\n",
    "# Create the classifiers for the ensemble\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Create the ensemble classifier using Voting Classifier\n",
    "ensemble_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Apply preprocessing to features\n",
    "    ('classifier', VotingClassifier(\n",
    "        estimators=[('rf', rf_classifier), ('gb', gb_classifier)], voting='hard'))\n",
    "])\n",
    "\n",
    "# Step 7: Train the Ensemble Model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Make Predictions\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Step 9: Evaluate the Model\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optional: Hyperparameter tuning or feature selection can be added to improve performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print column names to inspect\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"C:/Users/LENOVO/Desktop/bank.csv\"  # Path to your dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Inspect columns (already done)\n",
    "print(data.columns)\n",
    "\n",
    "# Step 3: Data Preprocessing\n",
    "target_column = 'deposit'  # The target variable column name\n",
    "X = data.drop(columns=[target_column])  # Features\n",
    "y = data[target_column]  # Target variable\n",
    "\n",
    "# Handling missing data (if any)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputing missing values with the most frequent value for categorical columns and mean for numerical columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Imputation for numerical features (mean)\n",
    "numerical_imputer = SimpleImputer(strategy='mean')\n",
    "X[numerical_features] = numerical_imputer.fit_transform(X[numerical_features])\n",
    "\n",
    "# Imputation for categorical features (most frequent)\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X[categorical_features] = categorical_imputer.fit_transform(X[categorical_features])\n",
    "\n",
    "# Step 4: Feature Scaling and Encoding\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessor for scaling numerical features and encoding categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Step 5: Build and Train Ensemble Model\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, LogisticRegression\n",
    "\n",
    "ensemble_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', VotingClassifier(estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('lr', LogisticRegression())\n",
    "    ], voting='hard'))\n",
    "])\n",
    "\n",
    "# Step 6: Train the model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predictions and Evaluation\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Accuracy and classification report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print column names to inspect\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
